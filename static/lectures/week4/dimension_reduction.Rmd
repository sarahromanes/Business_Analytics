---
title: "ETC3250: Dimension reduction"
subtitle: "Semester 1, 2020"
author: "<br> Professor Di Cook <br> <br> Econometrics and Business Statistics <br> Monash University"
date: "Week 4 (a)"
output: 
  xaringan::moon_reader:
    css: ["kunoichi", "ninjutsu", "mystyle.css", "libs/animate.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(tidy = FALSE, 
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE, 
                      fig.width=8,
                      fig.height=6,
                      fig.align = "center",
                      fig.retina = 4)
options(htmltools.dir.version = FALSE)
library(magick)
```

class: middle
background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/98/Andromeda_Galaxy_%28with_h-alpha%29.jpg)
background-position: 50% 50% class: center, bottom, inverse

.white[Space is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think it's a long way down the road to the chemist's, but that's just peanuts to space.] 

**.white[Douglas Adams, Hitchhiker's Guide to the Galaxy]**

---
## High Dimensional Data 

<br>
<br>

Remember, our data can be denoted as:

$\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^N, ~~~ \mbox{where}~ x_i = (x_{i1}, \dots, x_{ip})^{T}$
<br>
<br>

then

.tip[.orange[.content[Dimension ]] .content[of the data is *p*, ] .orange[.content[ the number of variables.]]]




---
## Cubes and Spheres


Space expands exponentially with dimension:

<img src="images/hypercube.png" style="width: 50%; align: center" />
<img src="images/cube_sphere.png" style="width: 30%; align: center" />

As dimension increases the .orange[volume of a sphere] of same radius as cube side length becomes much .orange[smaller than the volume of the cube].


---

## Examples of High Dimensional Data

High dimensional data occurs commonly in bioinformatics, when genetic studies often have many more information on genes than patients.

<center>
<img src="images/microarray.png" style="width: 70%; align: center"/>
</center>
---
class: split-two

.column[.pad50px[

## Example -  SRBCT cancer prediction 

- The SRBCT dataset (Khan et al., 2001) looks at classifying 4 classes of different childhood tumours sharing similar visual features during routine histology.
- Data contains 83 microarray samples with 1586 features.
- .orange[We will revisit this data later on in the course to explore high dimensional DA.]


]]

.column[.content.vmiddle.center[

 <img src="images/SRBCT-nature.jpg", width="70%">

.purple[Source:] [Nature](https://www.nature.com/articles/modpathol2016119)

]]

---
## Sub-spaces

<br>
<br>
Data will often be confined to a region of the space having lower .orange[intrinsic dimensionality]. The data lives in a low-dimensional subspace.

.orange[SO, reduce dimensionality], to the subspace containing the data.

---
## Principal Component Analysis (PCA)

<br>

.tip[Principal component analysis (PCA) produces a low-dimensional representation of a
dataset. It finds a sequence of linear combinations of the
variables that have .orange[maximal variance], and are .orange[mutually uncorrelated]. It is an unsupervised learning method. 
]



---

## Why use PCA?

<br>

- We may have too many predictors for a regression. Instead, we can use the first few principal components. 
- Understanding relationships between variables.
- Data visualization. We can plot a small number of variables more easily than a large number of variables.

---

## First principal component

The first principal component of a set of variables $x_1, x_2, \dots, x_p$ is the linear combination


$$z_1 = \phi_{11}x_1 + \phi_{21} x_2 + \dots + \phi_{p1} x_p$$


that has the largest variance such that  

$$\displaystyle\sum_{j=1}^p \phi^2_{j1} = 1$$

The elements $\phi_{11},\dots,\phi_{p1}$ are the .orange[loadings] of the first principal component.


---
## Geometry

- The loading vector $\phi_1 = [\phi_{11},\dots,\phi_{p1}]'$
defines direction in feature space along which data
vary most.
- If we project the $n$ data points ${x}_1,\dots,{x}_n$ onto this
direction, the projected values are the principal component
scores $z_{11},\dots,z_{n1}$.
- The second principal component is the linear combination $z_{i2} = \phi_{12}x_{i1} + \phi_{22}x_{i2} + \dots + \phi_{p2}x_{ip}$ that has maximal variance among all linear
combinations that are *uncorrelated* with $z_1$.
- Equivalent to constraining $\phi_2$ to be orthogonal (perpendicular) to $\phi_1$. And so on.
- There are at most $\min(n - 1, p)$ PCs.


---
## Example

<center>
<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.14.pdf" target="_BLANK"> <img src="images/6.14.png" style="width: 70%; align: center"/> </a>
</center>

.green[First PC]; .blue[second PC]

.font_tiny[(Chapter6/6.14.pdf)]



---
## Example

<center>

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.15.pdf" target="_BLANK"> <img src="images/6.15.png" style="width: 100%; align: center"/> </a>

</center>

If you think of the first few PCs like a linear model fit, and the others as the error, it is like regression, except that errors are orthogonal to model. 

.font_tiny[(Chapter6/6.15.pdf)]



---
## Computation


PCA can be thought of as fitting an $n$-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. The new variables produced by principal components correspond to .orange[rotating] and .orange[scaling] the ellipse .orange[into a circle].

```{r eval=FALSE}
library(tidyverse)
f.norm.vec<-function(x) {
  x<-x/f.norm(x)
  x
}
f.norm<-function(x) { sqrt(sum(x^2)) }
f.gen.sphere<-function(n=100,p=5) {
  x<-matrix(rnorm(n*p),ncol=p)
  xnew<-t(apply(x,1,f.norm.vec))
  xnew
}
f.vc.ellipse <- function(vc, xm, n=500) {
  p<-ncol(vc)
  x<-f.gen.sphere(n,p)

  evc<-eigen(vc)
  vc2<-(evc$vectors)%*%diag(sqrt(evc$values))%*%t(evc$vectors)
  x<-x%*%vc2

  x + matrix(rep(xm, each=n),ncol=p)
}
df <- f.vc.ellipse(vc=matrix(c(1,0.8,0.8,2), ncol=2), xm=c(0,0), n=1000)
df <- as_tibble(df)
ev <- tibble(e1=c(0.4847685, -0.8746425), e2=c(0.8746425, 0.4847685),
                group=c("pc1","pc2"))
```

```{r eval=FALSE}
library(gganimate)
ggplot(df, aes(x=V1, y=V2)) + geom_point() + 
  xlim(c(-1.5, 1.5)) + ylim(c(-1.5, 1.5)) + 
  geom_abline(data=ev, aes(intercept=0, slope=e2/e1, colour=group), size=2) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position="none") +
  transition_states(group, 1, 1) + enter_grow() + exit_shrink() +
  labs(title = "{closest_state}")
```

```{r eval=FALSE}
df_pca <- prcomp(df, retx=T, center=T, scale=TRUE)
df_lines <- tibble(V1=c(seq(-1,1, 0.5), seq(-1,1, 0.5), c(-1.5, 1.5), c(0, 0)), 
                   V2=c(seq(-1,1, 0.5)*ev$e2[1]/ev$e1[1], 
                        seq(-1,1, 0.5)*ev$e2[2]/ev$e1[2], c(0, 0), c(-1.5, 1.5)),
                   pc=c(rep("pc1", 5), rep("pc2", 5), "pc1", "pc1", "pc2", "pc2"),
                   group=c(rep("raw data", 10), rep("principal components", 4)))
df_pc <- as_tibble(df_pca$x) %>%
  rename(V1=PC1, V2=PC2) %>%
  mutate(V1 = V1/df_pca$sdev[1], V2 = V2/df_pca$sdev[2])
ggplot(df_pc, aes(x=V1, y=V2)) + geom_point() + theme(aspect.ratio=1)
all <- bind_rows(df, df_pc) %>% 
  mutate(group=c(rep("raw data", 1000), rep("principal components", 1000))) %>%
  mutate(group = factor(group, levels=c("raw data", "principal components")))
p <- ggplot(all, aes(x=V1, y=V2)) + geom_point(size=1) + 
  geom_line(data=df_lines, aes(x=V1, y=V2, group=interaction(group, pc), colour=pc)) +
  scale_colour_brewer("", palette="Dark2") +
  xlim(c(-2, 2)) + ylim(c(-2, 2)) + 
  xlab("") + ylab("") +
  theme(aspect.ratio=1) +
  transition_states(group, 1, 1) + enter_grow() + exit_shrink() +
  labs(title = "{closest_state}")
anim_save(filename = "images/pc-demo.gif", animation = p, 
          start_pause = 15, width = 480, height = 480, res = 150)
```

<center>
<img src="images/pc-demo.gif" style="width: 40%; align: center" />
</center>


---
## Computation

Suppose we have a $n\times p$ data set $X = [x_{ij}]$. 

- Centre each of the variables to have mean zero (i.e., the
column means of ${X}$ are zero).
-  $z_{i1} = \phi_{11}x_{i1} + \phi_{21} x_{i2} + \dots + \phi_{p1} x_{ip}$
- Sample variance of $z_{i1}$ is $\displaystyle\frac1n\sum_{i=1}^n z_{i1}^2$.


$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} \frac{1}{n}\sum_{i=1}^n 
\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right)^{\!\!\!2} \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$


---
## Computation

1. Compute the covariance matrix (after scaling the columns of ${X}$)
$${C} = {X}'{X}$$

2. Find eigenvalues and eigenvectors:
$${C}={V}{D}{V}'$$ 
where columns of ${V}$ are orthonormal (i.e., ${V}'{V}={I}$)

3. Compute PCs: ${\Phi} = {V}$. ${Z} = {X}{\Phi}$.


---

## Singular Value Decomposition

$$X = U\Lambda V'$$


- $X$ is an $n\times p$ matrix
- $U$ is $n \times r$ matrix with orthonormal columns ( $U'U=I$ )
- $\Lambda$ is $r \times r$ diagonal matrix with non-negative elements.
- $V$ is $p \times r$ matrix with orthonormal columns ( $V'V=I$ ).

.tip[
It is always possible to uniquely decompose a matrix in this way.
]

---
## Computation

1. Compute SVD: ${X} = {U}{\Lambda}{V}'$.

2. Compute PCs: ${\Phi} = {V}$. ${Z} = {X}{\Phi}$.

Relationship with covariance:

$${C} = {X}'{X}
       = {V}{\Lambda}{U}' {U}{\Lambda}{V}'
       = {V}{\Lambda}^2{V}'
       = {V}{D}{V}'$$
       

- Eigenvalues of ${C}$ are squares of singular values of ${X}$.
- Eigenvectors of ${C}$ are right singular vectors of ${X}$.
- The PC directions $\phi_1,\phi_2,\phi_3,\dots$ are the
right singular vectors of the matrix ${X}$.


---
## Total variance

.orange[Total variance] in data (assuming variables centered at 0):

$$\text{TV} = \sum_{j=1}^p \text{Var}(x_j) = \sum_{j=1}^p \frac{1}{n}\sum_{i=1}^n x_{ij}^2$$

.center[.green[**If variables are standardised, TV=number of variables!**]]



.orange[Variance explained] by *m*'th PC:

$$V_m = \text{Var}(z_m) = \frac{1}{n}\sum_{i=1}^n z_{im}^2$$
$$\text{TV} = \sum_{m=1}^M V_m \text{  where }M=\min(n-1,p).$$






---
layout: false
# `r set.seed(2019); emo::ji("technologist")` Made by a human with a computer

### Slides at [https://monba.dicook.org](https://monba.dicook.org).
### Code and data at [https://github.com/dicook/Business_Analytics](https://github.com/dicook/Business_Analytics).
<br>

### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

<br> 
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
